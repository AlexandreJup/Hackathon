{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hackatoche.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lk5UHAHsPA0c",
        "PKu86DY2B6SX",
        "dirORqwRGaX7",
        "Kh2_AGHIKNrA",
        "lN27Wyn6JAWk",
        "iXlYrn8zJD4P",
        "vuxGQRTDJj9H"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ick1u2tjI0mD",
        "colab_type": "text"
      },
      "source": [
        "# Data processing from raw\n",
        "BAUDIER Stéfan, FRERING Arnaud, JUPPET Alexandre, KNELL Nicolas, RODRIGUEZ Thomas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnpmMfYsIuYg",
        "colab_type": "text"
      },
      "source": [
        "##Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Eg4CxcoYIeE",
        "colab_type": "code",
        "outputId": "e0f116f9-7e11-4f43-f45a-0041108f75b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import math as m\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import trange\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  print(\"Not using COLAB\")\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers, losses\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor as rf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "print(\"TensorFlow version:\",tf.__version__)\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMUbn_AiJPGm",
        "colab_type": "text"
      },
      "source": [
        "##Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0KCIuKJgFDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "raw_train = pd.read_csv(\"https://raw.githubusercontent.com/AlexandreJup/Hackathon/master/train.csv\",sep=',')\n",
        "raw_test = pd.read_csv(\"https://raw.githubusercontent.com/AlexandreJup/Hackathon/master/test_student.csv\",sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2TWcScPJTv3",
        "colab_type": "text"
      },
      "source": [
        "##Data modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftRKtMb5j4BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod_raw_train = raw_train.drop(columns=['from','to', 'index', 'date'])\n",
        "mod_raw_test = raw_test.drop(columns=['from','to', 'index', 'date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyoTPfSHKmcB",
        "colab_type": "text"
      },
      "source": [
        "Fonctions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI3jZMELKluc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention les deux df data_train et data_test doivent être donnés sans le(s) label(s)\n",
        "# data_train correspond au pd.df d'entrainement (il faut que les normalisations, etc soient déjà faits)\n",
        "# data_test correspond au pd.df de test (le vrai sans )\n",
        "# column_keep est une liste de string des noms des colonnes à garder\n",
        "# column_dummies est une liste de string des noms des colonnes à transformer en 1-hot.\n",
        "# cette fonction renvoie 2 liste : l'entrainement et le test avec les bonnes colonnes et les 1-hot demandés\n",
        "\n",
        "def concat_deconcat(data_train,data_test, column_keep, column_dummies):\n",
        "  len_column_keep = len(column_keep)\n",
        "  len_column_dummies = len(column_dummies)\n",
        "  len_data_train = len(data_train)\n",
        "  len_data_test = len(data_test)\n",
        "  data_concat=pd.concat([data_train,data_test],sort=False)\n",
        "  data = data_concat.loc[:,column_keep]\n",
        "  data = pd.get_dummies(data,columns = column_dummies)\n",
        "  modified_train = data.iloc[:len_data_train]\n",
        "  modified_test = data.iloc[len_data_train:]\n",
        "  return (modified_train, modified_test)\n",
        "\n",
        "def send_to_csv(dataframe, file_name):\n",
        "  dataframe.to_csv(index=False, path_or_buf= file_name)\n",
        "\n",
        "\n",
        "\n",
        "def stacking_prep(list_of_models, list_of_files):\n",
        "  l = len(list_of_files)\n",
        "  list_of_df = [0 for _ in range(l)]\n",
        "  \n",
        "  for i in range(l):\n",
        "    list_of_files[i] = \"/content/drive/My Drive/Datasets Hackathon/\"+list_of_files[i]\n",
        "    list_of_df[i] = pd.read_csv(list_of_files[i])\n",
        "  input_for_stacking = pd.concat(list_of_df, axis=1, sort=False)\n",
        "  input_for_stacking = input_for_stacking.drop(columns = \"Id\")\n",
        "  input_for_stacking.columns = list_of_models\n",
        "  return(input_for_stacking)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g0zuZcbefsM",
        "colab_type": "text"
      },
      "source": [
        "Conversion of the scheduled_time in four informations:\n",
        "\n",
        "*   The month\n",
        "*   The day of the week\n",
        "*   The hour\n",
        "*   If the example is during weekend or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1B2ocLmeeHN",
        "colab_type": "code",
        "outputId": "046f61fa-c833-4cad-ec72-81e452760434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "nb_rows_train = len(mod_raw_train['scheduled_time'])\n",
        "Month_train = [0 for i in range(nb_rows_train)]\n",
        "Weekend_train = [0 for i in range(nb_rows_train)]\n",
        "Hour_train = [0 for i in range(nb_rows_train)]\n",
        "Day_train = [0 for i in range(nb_rows_train)]\n",
        "\n",
        "nb_rows_test = len(mod_raw_test['scheduled_time'])\n",
        "Month_test = [0 for i in range(nb_rows_test)]\n",
        "Weekend_test = [0 for i in range(nb_rows_test)]\n",
        "Hour_test = [0 for i in range(nb_rows_test)]\n",
        "Day_test = [0 for i in range(nb_rows_test)]\n",
        "\n",
        "for i in trange(nb_rows_train):\n",
        "  date = mod_raw_train['scheduled_time'][i]\n",
        "\n",
        "  # Defining the format of the date\n",
        "  format_date = '%Y-%m-%d %H:%M:%S'\n",
        "  datetime_obj = datetime.datetime.strptime(date, format_date)\n",
        "\n",
        "  # Defining which days are weekends\n",
        "  if datetime_obj.weekday() in [5, 6]:\n",
        "    Weekend_train[i] = 1\n",
        "  Month_train[i] = datetime_obj.month\n",
        "  Hour_train[i] = datetime_obj.hour\n",
        "  Day_train[i] = datetime_obj.weekday()\n",
        "\n",
        "for i in trange(nb_rows_test):\n",
        "  date = mod_raw_test['scheduled_time'][i]\n",
        "  format_date = '%Y-%m-%d %H:%M:%S'\n",
        "  datetime_obj = datetime.datetime.strptime(date, format_date)\n",
        "  if datetime_obj.weekday() in [5, 6]:\n",
        "    Weekend_test[i] = 1\n",
        "  Month_test[i] = datetime_obj.month\n",
        "  Hour_test[i] = datetime_obj.hour\n",
        "  Day_test[i] = datetime_obj.weekday()\n",
        "\n",
        "mod_raw_train['month'] = Month_train\n",
        "mod_raw_train['hour'] = Hour_train\n",
        "mod_raw_train['is_weekend'] = Weekend_train\n",
        "mod_raw_train['day'] = Day_train\n",
        "\n",
        "mod_raw_test['month'] = Month_test\n",
        "mod_raw_test['hour'] = Hour_test\n",
        "mod_raw_test['is_weekend'] = Weekend_test\n",
        "mod_raw_test['day'] = Day_test"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 418483/418483 [00:09<00:00, 44787.99it/s]\n",
            "100%|██████████| 179350/179350 [00:04<00:00, 44415.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S508pn7CKOAO",
        "colab_type": "text"
      },
      "source": [
        "## Normalisation of the data : (x-min)/(max-min)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz7DPn7u8qu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalisation\n",
        "#stop_sequence\n",
        "min_stop_sequence = min(min(mod_raw_train['stop_sequence']), min(mod_raw_test['stop_sequence']))\n",
        "max_stop_sequence = max(max(mod_raw_train['stop_sequence']), max(mod_raw_test['stop_sequence']))\n",
        "\n",
        "mod_raw_train['stop_sequence'] = (mod_raw_train['stop_sequence']-min_stop_sequence)/(max_stop_sequence-min_stop_sequence)\n",
        "mod_raw_test['stop_sequence'] = (mod_raw_test['stop_sequence']-min_stop_sequence)/(max_stop_sequence-min_stop_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6-I1a3KLAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming the 'estimated' status to 'departed'\n",
        "mod_raw_train.loc[mod_raw_train['status'] == 'estimated', 'status'] = 'departed'\n",
        "mod_raw_test.loc[mod_raw_test['status'] == 'estimated','status'] = 'departed'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCbyhYZEj5bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Features and labels\n",
        "feature_train = mod_raw_train.drop(columns=['delay_minutes'])\n",
        "feature_test = mod_raw_test\n",
        "\n",
        "# Defining the label \n",
        "label_train = mod_raw_train['delay_minutes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b4dr6BJLCut",
        "colab_type": "text"
      },
      "source": [
        "## Création du dataframe final\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLtgwZtQK5OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the 'concat_deconcat' function to choose which columns to keep and which ones to 1-hot encode\n",
        "training_features, testing_features = concat_deconcat(feature_train, feature_test, ['day','stop_sequence','line', 'hour','from_id', 'to_id'],['day','line','from_id', 'to_id', 'hour'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-ts4TrvPh6U",
        "colab_type": "text"
      },
      "source": [
        "#Partie Apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_NnaD0Pm2O",
        "colab_type": "text"
      },
      "source": [
        "##Partie Calcul RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAi6N09LoeHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into train and test subsets for training \n",
        "X_train, X_test, y_train, y_test = train_test_split(training_features, label_train, test_size=0.2,shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKlsEIxEvWEO",
        "colab_type": "text"
      },
      "source": [
        "###Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37squnavY9N",
        "colab_type": "code",
        "outputId": "2092bb25-3310-4956-af0f-db88a9c66d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# create regressor object \n",
        "regressor = rf(n_estimators = 20, max_depth = 100, random_state = 0) \n",
        "  \n",
        "# fit the regressor with x and y data \n",
        "regressor.fit(X_train, y_train)  \n",
        "\n",
        "# Use the forest's predict method on the test data\n",
        "predictions = regressor.predict(X_test)\n",
        "# Calculate the absolute errors\n",
        "nb_lines = len(X_test)\n",
        "error = 0\n",
        "for i in range(nb_lines):\n",
        "  error+= (predictions[i]-y_test.iloc[i,].values)**2\n",
        "error = np.sqrt(error/nb_lines)\n",
        "\n",
        "# Print out the RMSE\n",
        "print('RMSE:', error)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE: [6.7272565]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmJ1xHUPzwg",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL8Iqob5PzA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a random forest by splitting the dataset into subsets, separating by train line\n",
        "motif = 'line'\n",
        "\n",
        "list_motif_in_columns = []\n",
        "for name_column in X_train:\n",
        "  if motif in name_column:\n",
        "    list_motif_in_columns.append(name_column)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SWoNp5AP_7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_lines = len(list_motif_in_columns)\n",
        "list_rmse = [0 for i in range(0,nb_lines)]\n",
        "\n",
        "#Splitting the dataset into subsets of lines \n",
        "for i in range (0, nb_lines):\n",
        "  print(i)\n",
        "  targeted_train_lignes = X_train[list_motif_in_columns[i]]==1\n",
        "  df_train_features_i = X_train[targeted_train_lignes]\n",
        "  df_train_label_i = y_train[targeted_train_lignes]\n",
        "  df_train_features_i = df_train_features_i.drop(columns = list_motif_in_columns)\n",
        "\n",
        "  targeted_test_lignes = X_test[list_motif_in_columns[i]]==1\n",
        "  df_test_features_i = X_test[targeted_test_lignes]\n",
        "  df_test_label_i = y_test[targeted_test_lignes]\n",
        "  df_test_features_i = df_test_features_i.drop(columns = list_motif_in_columns)\n",
        "\n",
        "  # create regressor object \n",
        "  regressor = rf(n_estimators = 10, random_state = 0) \n",
        "  \n",
        "  # fit the regressor with x and y data \n",
        "  regressor.fit(df_train_features_i, df_train_label_i)  \n",
        "\n",
        "  # Use the forest's predict method on the test data\n",
        "  predictions = regressor.predict(df_test_features_i)\n",
        "  # Calculate the absolute errors\n",
        "  rmse = sqrt(mean_squared_error(df_test_label_i, predictions))\n",
        "\n",
        "  list_rmse[i] = rmse\n",
        "  \n",
        "print(list_rmse)\n",
        "print(np.mean(list_rmse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSNZ4OGQsJM6",
        "colab_type": "text"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj8cJ_OksIxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the MLP model, we choose this architecture and tried several layers combination\n",
        "\n",
        "class MLP(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.base_layer = tf.keras.layers.Dense(hidden_size, activation = 'relu')\n",
        "        self.dense_layer = tf.keras.layers.Dense(hidden_size, activation = 'relu')\n",
        "        self.regression = tf.keras.layers.Dense(output_size, activation='relu')\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.2)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.base_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.regression(x)\n",
        "\n",
        "\n",
        "model_MLP = MLP(hidden_size=64,output_size=1)\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 500\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "#We monitor the mean_squared_error for training\n",
        "loss = losses.mean_squared_error\n",
        "\n",
        "metrics = ['mse']\n",
        "\n",
        "model_MLP.compile(optimizer, loss=loss)\n",
        "\n",
        "# Training our model on the splitted dataset\n",
        "model_MLP.fit(X_train.values, y_train.values, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T37lSSYW9s3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "49082ec2-5219-4070-83c0-2b663b1d0fd0"
      },
      "source": [
        "# Evaluating our model by calculating the RMSE between y_pred and y_test\n",
        "# We need to change the format of y_pred because the output is a ist and not an array\n",
        "y_pred_train = model_MLP.predict(X_test)\n",
        "nb_predicts = len(y_pred_training)\n",
        "y_pred_training = [0 for i in range(nb_predicts)]\n",
        "for i in range(nb_predicts):\n",
        "  y_pred_training[i] = y_pred_train[i][0]\n",
        "  \n",
        "\n",
        "# final vector is y_pred_training\n",
        "RMSE = np.sqrt(mean_squared_error(y_test.values, y_pred_training))\n",
        "print(RMSE)\n",
        "\n",
        "\n",
        "# storing the training results into a dataframe, used later\n",
        "results_mlp_training = pd.DataFrame({'Id' : range(0,nb_predicts), \"Predicted\" : y_pred_training})"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "6.7541083255946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJswVN7fOw6-",
        "colab_type": "text"
      },
      "source": [
        "##Calcul des predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk5UHAHsPA0c",
        "colab_type": "text"
      },
      "source": [
        "### Randomforest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWWL8XZmOwUY",
        "colab_type": "code",
        "outputId": "36c94621-e0d3-484f-e94a-c56ef25f1562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "regressor = rf(n_estimators = 20, random_state = 0) \n",
        "regressor.fit(training_features, label_train)\n",
        "\n",
        "Y_pred = regressor.predict(testing_features)\n",
        "Y_pred_train = regressor.predict(training_features)\n",
        "\n",
        "results_rf_basic = pd.DataFrame({'Id' : range(0,len(Y_pred)), \"Predicted\" : Y_pred})\n",
        "results_rf_basic_training = pd.DataFrame({'Id' : range(0,len(Y_pred)), \"Predicted\" : Y_pred_train})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKu86DY2B6SX",
        "colab_type": "text"
      },
      "source": [
        "### Randomforest split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yd-7DZJEyHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions_split(df_trainer, df_to_predict, motif):\n",
        "\n",
        "  list_motif_in_columns = []\n",
        "  for name_column in df_trainer:\n",
        "    if motif in name_column:\n",
        "      list_motif_in_columns.append(name_column)\n",
        "  nb_lines = len(list_motif_in_columns)\n",
        "  l = len(df_to_predict)\n",
        "  id = [i for i in range(0,l)]\n",
        "  init_pred = [-1 for i in range(0,l)]\n",
        "  results = pd.DataFrame({ 'Id' : id, \"Predicted\" : init_pred })\n",
        "  for i in trange (0, nb_lines):\n",
        "\n",
        "    targeted_train_lignes = df_trainer[list_motif_in_columns[i]]==1\n",
        "    df_train_features_i = df_trainer[targeted_train_lignes]\n",
        "    df_train_label_i = label_train[targeted_train_lignes]\n",
        "    df_train_features_i = df_train_features_i.drop(columns = list_motif_in_columns)\n",
        "\n",
        "    targeted_test_lignes = df_to_predict[list_motif_in_columns[i]]==1\n",
        "    df_test_features_i = df_to_predict[targeted_test_lignes]\n",
        "    df_test_features_i = df_test_features_i.drop(columns = list_motif_in_columns)\n",
        "\n",
        "    # create regressor object \n",
        "    regressor = rf(n_estimators = 20, random_state = 0) \n",
        "    \n",
        "    # fit the regressor with x and y data \n",
        "    regressor.fit(df_train_features_i, df_train_label_i)  \n",
        "\n",
        "    # Use the forest's predict method on the test data\n",
        "    predictions = regressor.predict(df_test_features_i)\n",
        "    \n",
        "    results.loc[targeted_test_lignes, \"Predicted\"] = predictions\n",
        "  return(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loZNXcWIFStR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_rf_line_split = compute_predictions_split(training_features, testing_features, \"line\")\n",
        "results_rf_day_split = compute_predictions_split(training_features, testing_features, \"day\")\n",
        "\n",
        "results_rf_line_split_training = compute_predictions_split(training_features, training_features, \"line\")\n",
        "results_rf_day_split_training = compute_predictions_split(training_features, training_features, \"day\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirORqwRGaX7",
        "colab_type": "text"
      },
      "source": [
        "###MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF_VJxbh66aO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "615021af-bbfc-4591-f6de-417b73cd6156"
      },
      "source": [
        "# Predicting the delays on the real test dataset (and transforming the y_pred to the right format)\n",
        "y_pred_test = model_MLP.predict(testing_features)\n",
        "nb_lines = len(testing_features)\n",
        "y_pred_testing = [0 for i in range(nb_lines)]\n",
        "for i in range(nb_lines):\n",
        "  y_pred_testing[i] = y_pred_test[i][0]\n",
        "\n",
        "# storing the results in a dataframe\n",
        "results_mlp = pd.DataFrame({'Id' : range(0,nb_lines), \"Predicted\" : y_pred_testing})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2_AGHIKNrA",
        "colab_type": "text"
      },
      "source": [
        "###KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0OnRYvZKJA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "motif_1 = 'day'\n",
        "motif_2 = 'line'\n",
        "\n",
        "list_motif_1_in_columns = []\n",
        "for name_column in training_features:\n",
        "  if motif_1 in name_column:\n",
        "    list_motif_1_in_columns.append(name_column)\n",
        "\n",
        "list_motif_2_in_columns = []\n",
        "for name_column in training_features:\n",
        "  if motif_2 in name_column:\n",
        "    list_motif_2_in_columns.append(name_column)\n",
        "\n",
        "nb_motifs_1 = len(list_motif_1_in_columns)\n",
        "nb_motifs_2 = len(list_motif_2_in_columns)\n",
        "\n",
        "l = len(training_features)\n",
        "id = [i for i in range(0,l)]\n",
        "init_pred = [-1 for i in range(0,l)]\n",
        "results_KNN = pd.DataFrame({ 'Id' : id, \"Predicted\" : init_pred })\n",
        "\n",
        "for i in trange (0, nb_motifs_1):\n",
        "  for j in trange (0, nb_motifs_2):\n",
        "    \n",
        "    #Select the lignes in the train df with day==i and line==j\n",
        "    targeted_train_lignes = (training_features[list_motif_1_in_columns[i]]!=0) & (training_features[list_motif_2_in_columns[j]]!=0)\n",
        "    df_train_features_ij = training_features[targeted_train_lignes]\n",
        "    df_train_label_ij = label_train[targeted_train_lignes]\n",
        "    #Drop the line and day columns from the choosen train dataset\n",
        "    df_train_features_ij = df_train_features_ij.drop(columns = list_motif_1_in_columns)\n",
        "    df_train_features_ij = df_train_features_ij.drop(columns = list_motif_2_in_columns)\n",
        "\n",
        "    #Select the lignes in the test df with day==i and line==j\n",
        "    targeted_test_lignes = (training_features[list_motif_1_in_columns[i]]!=0) & (training_features[list_motif_2_in_columns[j]]!=0)\n",
        "    df_test_features_ij = training_features[targeted_test_lignes]\n",
        "    #Drop the line and day columns from the choosen test dataset\n",
        "    df_test_features_ij = df_test_features_ij.drop(columns = list_motif_1_in_columns)\n",
        "    df_test_features_ij = df_test_features_ij.drop(columns = list_motif_2_in_columns)\n",
        "\n",
        "    if ( (len(df_train_features_ij)!=0) & (len(df_test_features_ij)!=0)):\n",
        "      # create regressor object \n",
        "      regressor = rf(n_estimators = 10, random_state = 0) \n",
        "      \n",
        "      # fit the regressor with x and y data \n",
        "      regressor.fit(df_train_features_ij, df_train_label_ij)  \n",
        "\n",
        "      # Use the forest's predict method on the test data\n",
        "      predictions = regressor.predict(df_test_features_ij)\n",
        "      \n",
        "      results_KNN.loc[targeted_test_lignes, \"Predicted\"] = predictions\n",
        "    else:\n",
        "       if (len(df_train_features_ij)==0):\n",
        "         print('il manque dans le data train:')\n",
        "         print(list_motif_1_in_columns[i],list_motif_2_in_columns[j])\n",
        "       if (len(df_test_features_ij)==0):\n",
        "         print('il manque dans le data test:')\n",
        "         print(list_motif_1_in_columns[i],list_motif_2_in_columns[j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c6a_AVUWjmA",
        "colab_type": "text"
      },
      "source": [
        "###Stack the 4 methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8KRS_N-VxqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_stacking = pd.DataFrame({\"Predicted_basic\" : results_rf_10_basic['Predicted'], \"Predicted_line_split\" : results_rf_10_line_split['Predicted'], \"Predicted_day_split\" : results_rf_10_day_split['Predicted'], \"Predicted_MLP\"  : results_mlp['Predicted']})\n",
        "results_stacking_training = pd.DataFrame({\"Predicted_basic\" : results_rf_10_basic_training['Predicted'], \"Predicted_line_split\" : results_rf_10_line_split_training['Predicted'], \"Predicted_day_split\" : results_rf_10_day_split_training['Predicted'], \"Predicted_MLP\"  : results_mlp_training['Predicted']})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4CdyPAIS3Q",
        "colab_type": "text"
      },
      "source": [
        "Our best results were with the simplest meta classifier : the average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sc8MDe9H6GW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_av = (results_stacking['Predicted_basic']+results_stacking['Predicted_line_split']+results_stacking['Predicted_day_split']+results_stacking[\"Predicted_MLP\"])/4\n",
        "results_av = pd.DataFrame({'Id' : range(0,len(predictions_av)), \"Predicted\" : predictions_av})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4MjsdaYIM94",
        "colab_type": "text"
      },
      "source": [
        "But we also tried with more complex meta-classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN27Wyn6JAWk",
        "colab_type": "text"
      },
      "source": [
        "#####Linear regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbrDznsLH_dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regressor = LinearRegression().fit(results_stacking_training, label_train)\n",
        "predictions_lr = regressor.predict(results_stacking)\n",
        "results_lr = pd.DataFrame({'Id' : range(0,len(predictions_lr)), \"Predicted\" : predictions_lr})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXlYrn8zJD4P",
        "colab_type": "text"
      },
      "source": [
        "####KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MgvzaRgJIZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regressor = KNeighborsRegressor(n_neighbors=5).fit(results_stacking_training, label_train)\n",
        "predictions_knn = regressor.predict(results_stacking)\n",
        "results_knn = pd.DataFrame({'Id' : range(0,len(predictions_knn)), \"Predicted\" : predictions_knn})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxGQRTDJj9H",
        "colab_type": "text"
      },
      "source": [
        "####RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXKLVtpPJnDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regressor = rf(n_estimators = 20, random_state = 0).fit(results_stacking_training, label_train)\n",
        "predictions_rf = regressor.predict(results_stacking)\n",
        "results_rf = pd.DataFrame({'Id' : range(0,len(predictions_rf)), \"Predicted\" : predictions_rf})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgc-K59MKwZP",
        "colab_type": "text"
      },
      "source": [
        "#Hackathon report\n",
        "BAUDIER Stéfan, FRERING Arnaud, JUPPET Alexandre, KNELL Nicolas, RODRIGUEZ Thomas\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###1) The preprocessing\n",
        "\n",
        "\n",
        "For the preprocessing part, we made differents modifications to optimize our datasets.\n",
        "First, we removed six columns : from, to, `index`, `date`, `status` and `train_id`.\n",
        "From and to are replaced by the `from_id` and `to_id`.\n",
        "The index column doesn’t give us important information, as id is giving the same.\n",
        "The `date` is replaced by the `scheduled_time` because we can get the same information.\n",
        "The `status` of the train wasn’t leading to better results, we deduced that it was a source of overfitting so we removed it.\n",
        "The `train_id` needs to be transformed into 1-hot encoded but this operation adds too much columns and leads to overfitting.\n",
        "\n",
        "After that, we transformed `scheduled_time` into `day` and `hour` columns by using the datetime library. We tried to use the month as column but as the months during the training and the testing dataset are different, the generalisation is bad.\n",
        "\n",
        "Then we have normalized the stop-sequence column.\n",
        "\n",
        "We encoded the columns `from_id`, `to_id`, `day` and `hour` in one hot vectors. The idea was to applied that on the test and train datasets combined. By doing this method on the two datasets combined, we added the missing information from each dataset in the other one. For example, if a line is present in the testing dataset but not in the training one, we need the column of this line in order to build its one hot vector and its prediction.\n",
        "\n",
        "###2) The approaches : how the best parameters were selected\n",
        "\n",
        "For the random forest, we tried with 1,5, 10, 20, 50 and 100 trees. The improvement for the RMSE were not very convincing with many trees compared to the high computation time, so we chose to keep 20 trees.\n",
        "For the maximum depth of a tree, we chose 100 in order to get a satisfying RMSE.\n",
        "\n",
        "###3) A comparison of the methods and their results\n",
        "\n",
        "We tested several models, in a first approach we used simple models of machine learning in order to see their results.\n",
        "\n",
        "Linear regression : **score = 102894604476**\n",
        "\n",
        "Tree : **score =  8.839** (max depth of 100)\n",
        "\n",
        "kNN : **score = 8.586** (5 neighbours)\n",
        "\n",
        "The prediction of the linear regression was very bad and predicted some very high numbers, which is why the RMSE is exploding. For the tree and for kNN we get reasonable results, because they are more efficient for modelling complex phenomena.\n",
        "\n",
        "Those results are comforting us in the idea that complex models will have better results. The idea was then to try a deep model so we firstly thought of a RNN. We didn’t succeed in implementing it, so we made a MLP with 3 hidden layers, ReLu as the activation function and a layer of dropout in order to avoid the overfitting.\n",
        "\n",
        "MLP : **score = 8.271**\n",
        "\n",
        "This prediction is way better than the simple models but not enough satisfying yet.\n",
        "\n",
        "In order to improve the performance, we implemented some ensemble learning methods. First, a random forest composed of 20 trees and with a max depth of 100. For this model, we tried many configurations of datasets (only the lines as one hot vectors, only the days as one hot vectors and the whole dataset which had the best result presented here). We then wanted to compile those models in a stacking approach, with the 3 random forest as primary predictors and an average function as meta-classifier.\n",
        "\n",
        "**Random forest : 8.208**\n",
        "\n",
        "**Stacking : 8.182**\n",
        ">\n",
        "Those results were the best we could get even if we tried other methods like bagging or RNN that didn’t worked well.\n",
        "\n",
        "###4) Our group dynamic\n",
        "\n",
        "We worked on a shared Google Colab that was gathering our progress while everyone had a personal copy in order to try new things. Each time somebody found a good method or improved our model and datasets, he push the modifications on the shared file.\n",
        "\n",
        "How took place our collabotary work ?\n",
        "At the beginning we discussed about the dataset to try to improve it for a future use. Like a debate, we discussed about all the different ideas on a board and arrived to a consensus. \n",
        "Once the preprocessing had ended, each of us was in charge of a specific model. The idea was to split the work to test all the models we saw during lessons. After computing different methods, we kept the most accurate one, the random forest, and we tried to improve it with some ensemble learning methods. Our best result was obtained with the stacking of random forest.\n",
        "\n",
        "###5) Difficulties encountered\n",
        "\n",
        "The construction of the RNN and the MLP were difficult to implement for the data processing.\n",
        "Once the **8.18** score of RMSE was reached, it was difficult for us to have a real improvement despite our data processing and our model improvements. This was a result of finding new ideas once we tried all the models we know and use ensemble learning to improve them. The difficulty to reach a better RMSE can be also linked to our preprocessing and the choices we made.\n",
        "\n",
        "\n"
      ]
    }
  ]
}